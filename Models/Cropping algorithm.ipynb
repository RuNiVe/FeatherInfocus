{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c49d38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3926 images.\n",
      "0/3926 images processed...\n",
      "100/3926 images processed...\n",
      "200/3926 images processed...\n",
      "300/3926 images processed...\n",
      "400/3926 images processed...\n",
      "500/3926 images processed...\n",
      "600/3926 images processed...\n",
      "700/3926 images processed...\n",
      "800/3926 images processed...\n",
      "1000/3926 images processed...\n",
      "1100/3926 images processed...\n",
      "1200/3926 images processed...\n",
      "1300/3926 images processed...\n",
      "1400/3926 images processed...\n",
      "1500/3926 images processed...\n",
      "1600/3926 images processed...\n",
      "1700/3926 images processed...\n",
      "1800/3926 images processed...\n",
      "1900/3926 images processed...\n",
      "2000/3926 images processed...\n",
      "2100/3926 images processed...\n",
      "2200/3926 images processed...\n",
      "2300/3926 images processed...\n",
      "2400/3926 images processed...\n",
      "2500/3926 images processed...\n",
      "2600/3926 images processed...\n",
      "2700/3926 images processed...\n",
      "2800/3926 images processed...\n",
      "2900/3926 images processed...\n",
      "3000/3926 images processed...\n",
      "3100/3926 images processed...\n",
      "3200/3926 images processed...\n",
      "3300/3926 images processed...\n",
      "3400/3926 images processed...\n",
      "3500/3926 images processed...\n",
      "3600/3926 images processed...\n",
      "3700/3926 images processed...\n",
      "3800/3926 images processed...\n",
      "3900/3926 images processed...\n",
      "Done! Cropped images saved to: ../aml-2025-feathers-in-focus/train_images/cropped_train_images/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "\n",
    "# Crop training images using YOLOv8 model\n",
    "\n",
    "\n",
    "# Device selection (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Paths\n",
    "input_folder = \"../aml-2025-feathers-in-focus/train_images/train_images/\"\n",
    "output_folder = \"../aml-2025-feathers-in-focus/train_images/cropped_train_images/\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Load YOLOv8 model\n",
    "\n",
    "model = YOLO(\"yolov8s.pt\")\n",
    "model.to(device)\n",
    "\n",
    "# Helper: crop from bounding box\n",
    "\n",
    "def crop_with_bbox(image_path, bbox, save_path):\n",
    "    \"\"\"\n",
    "    bbox = [x1, y1, x2, y2]\n",
    "    \"\"\"\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    w, h = img.size\n",
    "\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    # ensure within bounds\n",
    "    x1 = max(0, int(x1))\n",
    "    y1 = max(0, int(y1))\n",
    "    x2 = min(w, int(x2))\n",
    "    y2 = min(h, int(y2))\n",
    "\n",
    "    cropped = img.crop((x1, y1, x2, y2))\n",
    "    cropped.save(save_path)\n",
    "\n",
    "\n",
    "\n",
    "# Process images\n",
    "\n",
    "image_files = [f for f in os.listdir(input_folder)\n",
    "               if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "\n",
    "print(f\"Found {len(image_files)} images.\")\n",
    "\n",
    "for i, filename in enumerate(image_files):\n",
    "    img_path = os.path.join(input_folder, filename)\n",
    "    out_path = os.path.join(output_folder, filename)\n",
    "\n",
    "    # Run YOLO inference\n",
    "    results = model.predict(img_path, device=str(device), verbose=False)\n",
    "\n",
    "    # Get detections\n",
    "    detections = results[0].boxes\n",
    "\n",
    "    if len(detections) == 0:\n",
    "        # If no bird found, copy the full image instead of cropping\n",
    "        Image.open(img_path).save(out_path)\n",
    "        continue\n",
    "\n",
    "    # YOLO box format: (x1, y1, x2, y2)\n",
    "    # Choose the largest box (max area)\n",
    "    boxes_xyxy = detections.xyxy.cpu().numpy()\n",
    "    areas = [(b[2] - b[0]) * (b[3] - b[1]) for b in boxes_xyxy]\n",
    "    largest_box = boxes_xyxy[areas.index(max(areas))]\n",
    "\n",
    "    crop_with_bbox(img_path, largest_box, out_path)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(f\"{i}/{len(image_files)} images processed...\")\n",
    "\n",
    "print(\"Done! Cropped images saved to:\", output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2417c902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4000 images.\n",
      "0/4000 images processed...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "\n",
    "# Crop test images using YOLOv8 model\n",
    "\n",
    "\n",
    "# Device selection (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Paths\n",
    "input_folder = \"../aml-2025-feathers-in-focus/test_images/test_images/\"\n",
    "output_folder = \"../aml-2025-feathers-in-focus/test_images/cropped_test_images/\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Load YOLOv8 model\n",
    "\n",
    "model = YOLO(\"yolov8s.pt\")\n",
    "model.to(device)\n",
    "\n",
    "# Helper: crop from bounding box\n",
    "\n",
    "def crop_with_bbox(image_path, bbox, save_path):\n",
    "    \"\"\"\n",
    "    bbox = [x1, y1, x2, y2]\n",
    "    \"\"\"\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    w, h = img.size\n",
    "\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    # ensure within bounds\n",
    "    x1 = max(0, int(x1))\n",
    "    y1 = max(0, int(y1))\n",
    "    x2 = min(w, int(x2))\n",
    "    y2 = min(h, int(y2))\n",
    "\n",
    "    cropped = img.crop((x1, y1, x2, y2))\n",
    "    cropped.save(save_path)\n",
    "\n",
    "\n",
    "\n",
    "# Process images\n",
    "\n",
    "image_files = [f for f in os.listdir(input_folder)\n",
    "               if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "\n",
    "print(f\"Found {len(image_files)} images.\")\n",
    "\n",
    "for i, filename in enumerate(image_files):\n",
    "    img_path = os.path.join(input_folder, filename)\n",
    "    out_path = os.path.join(output_folder, filename)\n",
    "\n",
    "    # Run YOLO inference\n",
    "    results = model.predict(img_path, device=str(device), verbose=False)\n",
    "\n",
    "    # Get detections\n",
    "    detections = results[0].boxes\n",
    "\n",
    "    if len(detections) == 0:\n",
    "        # If no bird found, copy the full image instead of cropping\n",
    "        Image.open(img_path).save(out_path)\n",
    "        continue\n",
    "\n",
    "    # YOLO box format: (x1, y1, x2, y2)\n",
    "    # Choose the largest box (max area)\n",
    "    boxes_xyxy = detections.xyxy.cpu().numpy()\n",
    "    areas = [(b[2] - b[0]) * (b[3] - b[1]) for b in boxes_xyxy]\n",
    "    largest_box = boxes_xyxy[areas.index(max(areas))]\n",
    "\n",
    "    crop_with_bbox(img_path, largest_box, out_path)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(f\"{i}/{len(image_files)} images processed...\")\n",
    "\n",
    "print(\"Done! Cropped images saved to:\", output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac1112f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "945f32ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Mask R-CNN...\n",
      "Processing images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3926/3926 [09:43<00:00,  6.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Cropped images saved to: ../aml-2025-feathers-in-focus/train_images/tightcut_train_images/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Tight crop\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --------------------------------------------------\n",
    "# PATHS\n",
    "# --------------------------------------------------\n",
    "\n",
    "SOURCE_DIR = \"../aml-2025-feathers-in-focus/train_images/train_images/\"\n",
    "DEST_DIR   = \"../aml-2025-feathers-in-focus/train_images/tightcut_train_images/\"\n",
    "\n",
    "os.makedirs(DEST_DIR, exist_ok=True)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# LOAD MODEL\n",
    "# --------------------------------------------------\n",
    "\n",
    "print(\"Loading Mask R-CNN...\")\n",
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "model.cuda()  # remove if no GPU\n",
    "\n",
    "# COCO label index for \"bird\"\n",
    "BIRD_CLASS_ID = 15\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# PROCESS HELPER\n",
    "# --------------------------------------------------\n",
    "\n",
    "def crop_with_mask(image, mask):\n",
    "    \"\"\"\n",
    "    Given an image and a boolean mask, return a tight crop around the mask.\n",
    "    mask must be H × W (boolean or 0/1).\n",
    "    \"\"\"\n",
    "    mask = mask.astype(np.uint8)\n",
    "\n",
    "    # bounding box from mask\n",
    "    ys, xs = np.where(mask == 1)\n",
    "    if len(xs) == 0 or len(ys) == 0:\n",
    "        return None\n",
    "\n",
    "    x_min, x_max = xs.min(), xs.max()\n",
    "    y_min, y_max = ys.min(), ys.max()\n",
    "\n",
    "    cropped = image[y_min:y_max+1, x_min:x_max+1]\n",
    "    return cropped\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# MAIN LOOP\n",
    "# --------------------------------------------------\n",
    "\n",
    "print(\"Processing images...\")\n",
    "\n",
    "for filename in tqdm(os.listdir(SOURCE_DIR)):\n",
    "    if not filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        continue\n",
    "\n",
    "    src_path = os.path.join(SOURCE_DIR, filename)\n",
    "    dst_path = os.path.join(DEST_DIR, filename)\n",
    "\n",
    "    # load image\n",
    "    image = cv2.imread(src_path)\n",
    "    if image is None:\n",
    "        continue\n",
    "\n",
    "    # convert to RGB for PyTorch\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    img_tensor = F.to_tensor(image_rgb).unsqueeze(0).cuda()\n",
    "\n",
    "    # inference\n",
    "    with torch.no_grad():\n",
    "        out = model(img_tensor)[0]\n",
    "\n",
    "    boxes = out[\"boxes\"].cpu().numpy()\n",
    "    labels = out[\"labels\"].cpu().numpy()\n",
    "    masks  = out[\"masks\"].cpu().numpy()  # shape [N, 1, H, W]\n",
    "\n",
    "    # filter for birds only\n",
    "    bird_indices = np.where(labels == BIRD_CLASS_ID)[0]\n",
    "    if len(bird_indices) == 0:\n",
    "        # no bird found → skip file\n",
    "        continue\n",
    "\n",
    "    # pick largest bird mask\n",
    "    best_idx = None\n",
    "    best_area = 0\n",
    "\n",
    "    for i in bird_indices:\n",
    "        mask = masks[i, 0] > 0.5\n",
    "        area = mask.sum()\n",
    "        if area > best_area:\n",
    "            best_area = area\n",
    "            best_idx = i\n",
    "\n",
    "    if best_idx is None:\n",
    "        continue\n",
    "\n",
    "    # get chosen mask\n",
    "    mask = (masks[best_idx, 0] > 0.5)\n",
    "\n",
    "    # crop tightly\n",
    "    cropped = crop_with_mask(image, mask)\n",
    "    if cropped is None:\n",
    "        continue\n",
    "\n",
    "    # save result\n",
    "    cv2.imwrite(dst_path, cropped)\n",
    "\n",
    "print(\"Done! Cropped images saved to:\", DEST_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ccb1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tight crop for test images\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --------------------------------------------------\n",
    "# PATHS\n",
    "# --------------------------------------------------\n",
    "\n",
    "SOURCE_DIR = \"../aml-2025-feathers-in-focus/train_images/test_images/\"\n",
    "DEST_DIR   = \"../aml-2025-feathers-in-focus/train_images/tightcut_test_images/\"\n",
    "\n",
    "os.makedirs(DEST_DIR, exist_ok=True)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# LOAD MODEL\n",
    "# --------------------------------------------------\n",
    "\n",
    "print(\"Loading Mask R-CNN...\")\n",
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "model.cuda()  # remove if no GPU\n",
    "\n",
    "# COCO label index for \"bird\"\n",
    "BIRD_CLASS_ID = 15\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# PROCESS HELPER\n",
    "# --------------------------------------------------\n",
    "\n",
    "def crop_with_mask(image, mask):\n",
    "    \"\"\"\n",
    "    Given an image and a boolean mask, return a tight crop around the mask.\n",
    "    mask must be H × W (boolean or 0/1).\n",
    "    \"\"\"\n",
    "    mask = mask.astype(np.uint8)\n",
    "\n",
    "    # bounding box from mask\n",
    "    ys, xs = np.where(mask == 1)\n",
    "    if len(xs) == 0 or len(ys) == 0:\n",
    "        return None\n",
    "\n",
    "    x_min, x_max = xs.min(), xs.max()\n",
    "    y_min, y_max = ys.min(), ys.max()\n",
    "\n",
    "    cropped = image[y_min:y_max+1, x_min:x_max+1]\n",
    "    return cropped\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# MAIN LOOP\n",
    "# --------------------------------------------------\n",
    "\n",
    "print(\"Processing images...\")\n",
    "\n",
    "for filename in tqdm(os.listdir(SOURCE_DIR)):\n",
    "    if not filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        continue\n",
    "\n",
    "    src_path = os.path.join(SOURCE_DIR, filename)\n",
    "    dst_path = os.path.join(DEST_DIR, filename)\n",
    "\n",
    "    # load image\n",
    "    image = cv2.imread(src_path)\n",
    "    if image is None:\n",
    "        continue\n",
    "\n",
    "    # convert to RGB for PyTorch\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    img_tensor = F.to_tensor(image_rgb).unsqueeze(0).cuda()\n",
    "\n",
    "    # inference\n",
    "    with torch.no_grad():\n",
    "        out = model(img_tensor)[0]\n",
    "\n",
    "    boxes = out[\"boxes\"].cpu().numpy()\n",
    "    labels = out[\"labels\"].cpu().numpy()\n",
    "    masks  = out[\"masks\"].cpu().numpy()  # shape [N, 1, H, W]\n",
    "\n",
    "    # filter for birds only\n",
    "    bird_indices = np.where(labels == BIRD_CLASS_ID)[0]\n",
    "    if len(bird_indices) == 0:\n",
    "        # no bird found → skip file\n",
    "        continue\n",
    "\n",
    "    # pick largest bird mask\n",
    "    best_idx = None\n",
    "    best_area = 0\n",
    "\n",
    "    for i in bird_indices:\n",
    "        mask = masks[i, 0] > 0.5\n",
    "        area = mask.sum()\n",
    "        if area > best_area:\n",
    "            best_area = area\n",
    "            best_idx = i\n",
    "\n",
    "    if best_idx is None:\n",
    "        continue\n",
    "\n",
    "    # get chosen mask\n",
    "    mask = (masks[best_idx, 0] > 0.5)\n",
    "\n",
    "    # crop tightly\n",
    "    cropped = crop_with_mask(image, mask)\n",
    "    if cropped is None:\n",
    "        continue\n",
    "\n",
    "    # save result\n",
    "    cv2.imwrite(dst_path, cropped)\n",
    "\n",
    "print(\"Done! Cropped images saved to:\", DEST_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
