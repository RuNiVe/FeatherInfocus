{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c49d38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3926 images.\n",
      "0/3926 images processed...\n",
      "100/3926 images processed...\n",
      "200/3926 images processed...\n",
      "300/3926 images processed...\n",
      "400/3926 images processed...\n",
      "500/3926 images processed...\n",
      "600/3926 images processed...\n",
      "700/3926 images processed...\n",
      "800/3926 images processed...\n",
      "1000/3926 images processed...\n",
      "1100/3926 images processed...\n",
      "1200/3926 images processed...\n",
      "1300/3926 images processed...\n",
      "1400/3926 images processed...\n",
      "1500/3926 images processed...\n",
      "1600/3926 images processed...\n",
      "1700/3926 images processed...\n",
      "1800/3926 images processed...\n",
      "1900/3926 images processed...\n",
      "2000/3926 images processed...\n",
      "2100/3926 images processed...\n",
      "2200/3926 images processed...\n",
      "2300/3926 images processed...\n",
      "2400/3926 images processed...\n",
      "2500/3926 images processed...\n",
      "2600/3926 images processed...\n",
      "2700/3926 images processed...\n",
      "2800/3926 images processed...\n",
      "2900/3926 images processed...\n",
      "3000/3926 images processed...\n",
      "3100/3926 images processed...\n",
      "3200/3926 images processed...\n",
      "3300/3926 images processed...\n",
      "3400/3926 images processed...\n",
      "3500/3926 images processed...\n",
      "3600/3926 images processed...\n",
      "3700/3926 images processed...\n",
      "3800/3926 images processed...\n",
      "3900/3926 images processed...\n",
      "Done! Cropped images saved to: ../aml-2025-feathers-in-focus/train_images/cropped_train_images/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "\n",
    "# Crop training images using YOLOv8 model\n",
    "\n",
    "\n",
    "# Device selection (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Paths\n",
    "input_folder = \"../aml-2025-feathers-in-focus/train_images/train_images/\"\n",
    "output_folder = \"../aml-2025-feathers-in-focus/train_images/cropped_train_images/\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Load YOLOv8 model\n",
    "\n",
    "model = YOLO(\"yolov8s.pt\")\n",
    "model.to(device)\n",
    "\n",
    "# Helper: crop from bounding box\n",
    "\n",
    "def crop_with_bbox(image_path, bbox, save_path):\n",
    "    \"\"\"\n",
    "    bbox = [x1, y1, x2, y2]\n",
    "    \"\"\"\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    w, h = img.size\n",
    "\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    # ensure within bounds\n",
    "    x1 = max(0, int(x1))\n",
    "    y1 = max(0, int(y1))\n",
    "    x2 = min(w, int(x2))\n",
    "    y2 = min(h, int(y2))\n",
    "\n",
    "    cropped = img.crop((x1, y1, x2, y2))\n",
    "    cropped.save(save_path)\n",
    "\n",
    "\n",
    "\n",
    "# Process images\n",
    "\n",
    "image_files = [f for f in os.listdir(input_folder)\n",
    "               if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "\n",
    "print(f\"Found {len(image_files)} images.\")\n",
    "\n",
    "for i, filename in enumerate(image_files):\n",
    "    img_path = os.path.join(input_folder, filename)\n",
    "    out_path = os.path.join(output_folder, filename)\n",
    "\n",
    "    # Run YOLO inference\n",
    "    results = model.predict(img_path, device=str(device), verbose=False)\n",
    "\n",
    "    # Get detections\n",
    "    detections = results[0].boxes\n",
    "\n",
    "    if len(detections) == 0:\n",
    "        # If no bird found, copy the full image instead of cropping\n",
    "        Image.open(img_path).save(out_path)\n",
    "        continue\n",
    "\n",
    "    # YOLO box format: (x1, y1, x2, y2)\n",
    "    # Choose the largest box (max area)\n",
    "    boxes_xyxy = detections.xyxy.cpu().numpy()\n",
    "    areas = [(b[2] - b[0]) * (b[3] - b[1]) for b in boxes_xyxy]\n",
    "    largest_box = boxes_xyxy[areas.index(max(areas))]\n",
    "\n",
    "    crop_with_bbox(img_path, largest_box, out_path)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(f\"{i}/{len(image_files)} images processed...\")\n",
    "\n",
    "print(\"Done! Cropped images saved to:\", output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2417c902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4000 images.\n",
      "0/4000 images processed...\n",
      "100/4000 images processed...\n",
      "200/4000 images processed...\n",
      "300/4000 images processed...\n",
      "400/4000 images processed...\n",
      "500/4000 images processed...\n",
      "600/4000 images processed...\n",
      "700/4000 images processed...\n",
      "800/4000 images processed...\n",
      "900/4000 images processed...\n",
      "1000/4000 images processed...\n",
      "1100/4000 images processed...\n",
      "1200/4000 images processed...\n",
      "1300/4000 images processed...\n",
      "1400/4000 images processed...\n",
      "1500/4000 images processed...\n",
      "1600/4000 images processed...\n",
      "1700/4000 images processed...\n",
      "1800/4000 images processed...\n",
      "1900/4000 images processed...\n",
      "2000/4000 images processed...\n",
      "2100/4000 images processed...\n",
      "2200/4000 images processed...\n",
      "2300/4000 images processed...\n",
      "2400/4000 images processed...\n",
      "2500/4000 images processed...\n",
      "2600/4000 images processed...\n",
      "2700/4000 images processed...\n",
      "2800/4000 images processed...\n",
      "2900/4000 images processed...\n",
      "3000/4000 images processed...\n",
      "3100/4000 images processed...\n",
      "3200/4000 images processed...\n",
      "3300/4000 images processed...\n",
      "3400/4000 images processed...\n",
      "3500/4000 images processed...\n",
      "3600/4000 images processed...\n",
      "3700/4000 images processed...\n",
      "3800/4000 images processed...\n",
      "3900/4000 images processed...\n",
      "Done! Cropped images saved to: ../aml-2025-feathers-in-focus/test_images/cropped_test_images/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "\n",
    "# Crop test images using YOLOv8 model\n",
    "\n",
    "\n",
    "# Device selection (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Paths\n",
    "input_folder = \"../aml-2025-feathers-in-focus/test_images/test_images/\"\n",
    "output_folder = \"../aml-2025-feathers-in-focus/test_images/cropped_test_images/\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Load YOLOv8 model\n",
    "\n",
    "model = YOLO(\"yolov8s.pt\")\n",
    "model.to(device)\n",
    "\n",
    "# Helper: crop from bounding box\n",
    "\n",
    "def crop_with_bbox(image_path, bbox, save_path):\n",
    "    \"\"\"\n",
    "    bbox = [x1, y1, x2, y2]\n",
    "    \"\"\"\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    w, h = img.size\n",
    "\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    # ensure within bounds\n",
    "    x1 = max(0, int(x1))\n",
    "    y1 = max(0, int(y1))\n",
    "    x2 = min(w, int(x2))\n",
    "    y2 = min(h, int(y2))\n",
    "\n",
    "    cropped = img.crop((x1, y1, x2, y2))\n",
    "    cropped.save(save_path)\n",
    "\n",
    "\n",
    "\n",
    "# Process images\n",
    "\n",
    "image_files = [f for f in os.listdir(input_folder)\n",
    "               if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "\n",
    "print(f\"Found {len(image_files)} images.\")\n",
    "\n",
    "for i, filename in enumerate(image_files):\n",
    "    img_path = os.path.join(input_folder, filename)\n",
    "    out_path = os.path.join(output_folder, filename)\n",
    "\n",
    "    # Run YOLO inference\n",
    "    results = model.predict(img_path, device=str(device), verbose=False)\n",
    "\n",
    "    # Get detections\n",
    "    detections = results[0].boxes\n",
    "\n",
    "    if len(detections) == 0:\n",
    "        # If no bird found, copy the full image instead of cropping\n",
    "        Image.open(img_path).save(out_path)\n",
    "        continue\n",
    "\n",
    "    # YOLO box format: (x1, y1, x2, y2)\n",
    "    # Choose the largest box (max area)\n",
    "    boxes_xyxy = detections.xyxy.cpu().numpy()\n",
    "    areas = [(b[2] - b[0]) * (b[3] - b[1]) for b in boxes_xyxy]\n",
    "    largest_box = boxes_xyxy[areas.index(max(areas))]\n",
    "\n",
    "    crop_with_bbox(img_path, largest_box, out_path)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(f\"{i}/{len(image_files)} images processed...\")\n",
    "\n",
    "print(\"Done! Cropped images saved to:\", output_folder)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
