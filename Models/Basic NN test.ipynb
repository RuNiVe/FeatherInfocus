{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "75936d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "#Set GPU as device to use.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "0fd71e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../aml-2025-feathers-in-focus/train_images.csv\")\n",
    "df_train.head()\n",
    "\n",
    "complete_bird_attributes = pd.read_csv(\"complete_bird_attributes.csv\", index_col='class_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "cc211ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has_bill_shape::curved_(up_or_down)    0\n",
      "has_bill_shape::dagger                 0\n",
      "has_bill_shape::hooked                 0\n",
      "has_bill_shape::needle                 0\n",
      "has_bill_shape::hooked_seabird         1\n",
      "                                      ..\n",
      "has_crown_color::buff                  0\n",
      "has_wing_pattern::solid                1\n",
      "has_wing_pattern::spotted              0\n",
      "has_wing_pattern::striped              0\n",
      "has_wing_pattern::multi-colored        0\n",
      "Name: 1, Length: 312, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "num_concepts = complete_bird_attributes.shape[1]\n",
    "\n",
    "# test\n",
    "example_label = df_train[\"label\"].iloc[0]\n",
    "example_concepts = complete_bird_attributes.loc[example_label]\n",
    "print(example_concepts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d8cb372a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdConceptDataset(Dataset):\n",
    "    def __init__(self, csv_df, attributes_df, images_root):\n",
    "        self.df = csv_df\n",
    "        self.attributes = attributes_df\n",
    "        self.images_root = images_root\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load label\n",
    "        label = self.df.iloc[idx][\"label\"]\n",
    "        \n",
    "        # Load concept vector\n",
    "        concept_vec = torch.tensor(self.attributes.loc[label].values, dtype=torch.float32)\n",
    "        \n",
    "        # Build full image path\n",
    "        img_rel_path = self.df.iloc[idx][\"image_path\"]\n",
    "        img_path = os.path.join(self.images_root, os.path.basename(img_rel_path))\n",
    "\n",
    "        # Load image using your method\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "\n",
    "        return image, concept_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92001dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_dir = \"../aml-2025-feathers-in-focus/train_images/cropped_train_images/\"\n",
    "train_images = [\n",
    "    os.path.join(train_images_dir, f)\n",
    "    for f in os.listdir(train_images_dir)\n",
    "    if f.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp'))\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "764fcc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BirdConceptDataset(\n",
    "    csv_df=df_train,\n",
    "    attributes_df=complete_bird_attributes,\n",
    "    images_root=train_images_dir\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8a452a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BirdCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # -------- BLOCK 1 --------\n",
    "        # Input: 3 x 224 x 224\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(32)\n",
    "        self.act1  = nn.LeakyReLU(0.1, inplace=True)\n",
    "        self.pool1 = nn.MaxPool2d(2)  # -> 32 x 112 x 112\n",
    "\n",
    "        # -------- BLOCK 2 --------\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(64)\n",
    "        self.act2  = nn.LeakyReLU(0.1, inplace=True)\n",
    "        self.pool2 = nn.MaxPool2d(2)  # -> 64 x 56 x 56\n",
    "\n",
    "        # -------- BLOCK 3 --------\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn3   = nn.BatchNorm2d(128)\n",
    "        self.act3  = nn.LeakyReLU(0.1, inplace=True)\n",
    "        self.pool3 = nn.MaxPool2d(2)  # -> 128 x 28 x 28\n",
    "\n",
    "        # -------- BLOCK 4 --------\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn4   = nn.BatchNorm2d(256)\n",
    "        self.act4  = nn.LeakyReLU(0.1, inplace=True)\n",
    "        self.pool4 = nn.MaxPool2d(2)  # -> 256 x 14 x 14\n",
    "\n",
    "        # Global pool -> 256 features\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        self.fc1 = nn.Linear(256, 512)\n",
    "        self.act_fc1 = nn.LeakyReLU(0.1, inplace=True)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.act1(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(self.act2(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(self.act3(self.bn3(self.conv3(x))))\n",
    "        x = self.pool4(self.act4(self.bn4(self.conv4(x))))\n",
    "\n",
    "        x = self.gap(x)\n",
    "        x = torch.flatten(x, 1)  # -> (B, 256)\n",
    "\n",
    "        #head\n",
    "        x= self.act_fc1(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x= self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "04d283ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConceptNet(nn.Module):\n",
    "    def __init__(self, num_outputs=312):\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone = BirdCNN()\n",
    "\n",
    "        self.fc1 = nn.Linear(256, 512)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(512)\n",
    "        self.act_fc1 = nn.LeakyReLU(0.1, inplace=True)\n",
    "        self.drop1 = nn.Dropout(0.25)\n",
    "\n",
    "        self.fc2 = nn.Linear(512, num_outputs)   # logits\n",
    "        # No sigmoid here → use BCEWithLogitsLoss externally\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.drop1(self.act_fc1(self.bn_fc1(self.fc1(x))))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "14c3f0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConceptNet(num_outputs=num_concepts)\n",
    "model = model.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()    # output is already sigmoid\n",
    "criterion = criterion.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a6dd459d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 17367.8976\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[144]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m     loss.backward()\n\u001b[32m     16\u001b[39m     optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     total_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     22\u001b[39m torch.save(model.state_dict(), \u001b[33m\"\u001b[39m\u001b[33mbasic_best_model.pth\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        preds = model(images)         # predictions (batch_size × num_concepts)\n",
    "        loss = criterion(preds, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"basic_best_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333ddbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"best_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddde615",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_dir = \"../aml-2025-feathers-in-focus/test_images/cropped_test_images/\"\n",
    "\n",
    "test_images = [\n",
    "    os.path.join(test_images_dir, f)\n",
    "    for f in os.listdir(test_images_dir)\n",
    "    if f.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp'))\n",
    "]\n",
    "\n",
    "print(f\"Found {len(test_images)} test images\")\n",
    "\n",
    "\n",
    "img_size = 224\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# make sure model structure matches training\n",
    "model = ConceptNet(num_outputs=complete_bird_attributes.shape[1])\n",
    "model.load_state_dict(torch.load(\"best_model.pth\", map_location=device))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "num_concepts = len(complete_bird_attributes.columns)\n",
    "topk = 2  # number of concepts to keep per image\n",
    "\n",
    "pred_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for img_path in tqdm(test_images, desc=\"Predicting test images with Top-2\"):\n",
    "        # Load image\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        x = test_transform(img).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(x)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy().squeeze()  # shape (num_concepts,)\n",
    "        \n",
    "        # Initialize all zeros\n",
    "        topk_vec = np.zeros_like(probs)\n",
    "        \n",
    "        # Find top-2 indices\n",
    "        topk_idx = probs.argsort()[-topk:][::-1]\n",
    "        topk_vec[topk_idx] = 1\n",
    "        \n",
    "        pred_list.append(topk_vec)\n",
    "\n",
    "\n",
    "pred_array = np.stack(pred_list, axis=0)  # shape: (4000, 312)\n",
    "\n",
    "# Use column names from complete_bird_attributes\n",
    "pred_df = pd.DataFrame(pred_array, columns=complete_bird_attributes.columns)\n",
    "pred_df.insert(0, \"image_path\", [os.path.basename(f) for f in test_images])\n",
    "\n",
    "print(pred_df.shape)  # should be (4000, 312)\n",
    "\n",
    "pred_df.head()\n",
    "\n",
    "\n",
    "pred_df.to_csv(\"test_predictions.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b99a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN with K=1 on the predicted concept vectors to find the nearest training image\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Drop image_path column for NN, only keep concept vectors\n",
    "test_features = pred_df.drop(columns=[\"image_path\"]).values  # shape (4000, 312)\n",
    "\n",
    "# Make sure training attribute matrix is aligned (columns in same order)\n",
    "train_features = complete_bird_attributes.values  # shape (num_birds, 312)\n",
    "train_index = complete_bird_attributes.index.values  # class_key values\n",
    "\n",
    "# Fit Nearest Neighbors model\n",
    "k = 1\n",
    "nn_model = NearestNeighbors(n_neighbors=k, metric='euclidean')  # can use 'cosine' or 'euclidean'\n",
    "nn_model.fit(train_features)\n",
    "\n",
    "\n",
    "distances, indices = nn_model.kneighbors(test_features)  # indices shape (4000, k)\n",
    "\n",
    "nearest_class_keys = train_index[indices[:, 0]]  # shape (4000,)\n",
    "output_df = pd.DataFrame({\n",
    "    \"image_path\": pred_df[\"image_path\"].values,\n",
    "    \"class_key\": nearest_class_keys\n",
    "})\n",
    "\n",
    "output_df.head()\n",
    "\n",
    "output_df.to_csv(\"test_nearest_neighbors.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90734879",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_bird_attributes.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fc6217",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
